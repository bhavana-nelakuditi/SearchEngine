{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cb7f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name: Bhavana Nelakuditi\n",
    "# UIN: 667225823\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea7c120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and deploying a web crawler\n",
    "# Domain limitation for web agent\n",
    "domain = \"uic.edu\"\n",
    "start_url = \"https://cs.uic.edu/\"\n",
    "# Index number for the crawler\n",
    "crawl_index = 6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d8f40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to store the pages crawled\n",
    "pages_folder = \"./CrawledPages/\"\n",
    "error_file = \"error.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa705cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "extensions = [\".pdf\", \".doc\", \".docx\", \".ppt\", \".pptx\", \".xls\", \".xlsx\", \".css\", \".js\", \".aspx\", \".png\", \".jpg\", \".jpeg\", \".gif\", \".svg\", \".ico\", \".mp4\", \".avi\", \".tar\", \".gz\", \".tgz\", \".zip\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69dca51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Queue to maintain the webpages list for breadth first search traversal\n",
    "webpage_queue = deque()\n",
    "webpage_queue.append(start_url)\n",
    "crawl_urls = []\n",
    "crawl_urls.append(start_url)\n",
    "pages_crawled = {}\n",
    "page_no = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc0434a",
   "metadata": {},
   "outputs": [],
   "source": [
    "while webpage_queue:\n",
    "\n",
    "    try:\n",
    "        # Get the url at index position\n",
    "        url = webpage_queue.popleft()\n",
    "        rqst = requests.get(url)\n",
    "\n",
    "        if rqst.status_code == 200:\n",
    "            soup = BeautifulSoup(rqst.text, \"lxml\")\n",
    "            tags_extracted = soup.find_all(\"a\")\n",
    "\n",
    "            if len(tags_extracted) != 0:\n",
    "                pages_crawled[page_no] = url\n",
    "                output_file = pages_folder + str(page_no)\n",
    "\n",
    "                os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "                with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "                    file.write(rqst.text)\n",
    "                file.close()\n",
    "\n",
    "                for tag in tags_extracted:\n",
    "                    link = tag.get(\"href\")\n",
    "\n",
    "                    if (\n",
    "                        link is not None\n",
    "                        and link.startswith(\"http\")\n",
    "                        and not any(ext in link.lower() for ext in extensions)\n",
    "                    ):\n",
    "\n",
    "                        link = link.lower()\n",
    "                        link = link.split(\"#\")[0]\n",
    "                        link = link.split(\"?\", maxsplit=1)[0]\n",
    "                        link = link.rstrip(\"/\")\n",
    "                        link = link.strip()\n",
    "\n",
    "                        if link not in crawl_urls and domain in link:\n",
    "                            webpage_queue.append(link)\n",
    "                            crawl_urls.append(link)\n",
    "\n",
    "                if len(pages_crawled) > crawl_index:\n",
    "                    break\n",
    "\n",
    "                page_no += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        with open(error_file, \"a+\") as log:\n",
    "            log.write(f\"Coonection Failed for  {url}\")\n",
    "            log.write(f\"{e}\\n\\n\")\n",
    "        log.close()\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a07ec52",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_directory = \"./PickleFiles/\"\n",
    "os.makedirs(pickle_directory, exist_ok=True)\n",
    "with open(pickle_directory + \"crawled_pages.pickle\", \"wb\") as f:\n",
    "    pickle.dump(pages_crawled, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
